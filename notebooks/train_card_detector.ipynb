{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¥ Pokemon Card Detection - YOLO11 Training Notebook\n",
    "\n",
    "## What You'll Learn\n",
    "This notebook will teach you how to train a **YOLO11** object detection model to find Pokemon cards in images. By the end, you'll have:\n",
    "\n",
    "1. A trained model that draws bounding boxes around Pokemon cards\n",
    "2. Understanding of how YOLO training works\n",
    "3. A model ready to use with PriceLens\n",
    "\n",
    "## Dataset: Cleveland's 5K Real-World Images\n",
    "We're using the **Pokemon Card Identification** dataset from Roboflow:\n",
    "- **5,058 real-world photos** (not clean scans!)\n",
    "- Cards held in hands, on desks, in binders\n",
    "- Various lighting conditions and angles\n",
    "- Multiple cards per image scenarios\n",
    "- License: CC BY 4.0\n",
    "\n",
    "This matches what your webcam will actually see - much better than training on clean card scans!\n",
    "\n",
    "## Detection vs Identification\n",
    "- **Detection** (this notebook): Finding WHERE cards are in an image ‚Üí Bounding boxes\n",
    "- **Identification** (separate step): Figuring out WHICH card it is ‚Üí \"Base Set Charizard\"\n",
    "\n",
    "Think of it like this: Detection is like a security camera finding \"there's a person at coordinates (x,y)\". Identification is recognizing \"that's Bob from accounting\".\n",
    "\n",
    "## Why YOLO11?\n",
    "YOLO (You Only Look Once) is a family of fast object detection models. YOLO11 is the latest version with:\n",
    "- **Speed**: 30+ FPS on modern GPUs\n",
    "- **Accuracy**: State-of-the-art detection performance\n",
    "- **Simplicity**: Easy to train and deploy\n",
    "\n",
    "We'll use **YOLO11n** (nano) - the smallest and fastest variant, perfect for real-time applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup\n",
    "\n",
    "First, let's check if we're running in Google Colab or locally, then install the required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "import cv2, torch, ultralytics\n",
    "print(f\"\\n‚úì OpenCV: {cv2.__version__}\")\n",
    "print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úì YOLO: {ultralytics.__version__}\")\n",
    "print(f\"‚úì CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 1: Environment Setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"üñ•Ô∏è  Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "\n",
    "# Install dependencies\n",
    "if IN_COLAB:\n",
    "    print(\"üì¶ Installing dependencies for Colab...\")\n",
    "    %pip install -q ultralytics roboflow opencv-python matplotlib pandas\n",
    "else:\n",
    "    print(\"üì¶ Using local environment - ensure requirements.txt is installed\")\n",
    "    # For local: pip install ultralytics roboflow opencv-python matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"\\nüîß Hardware Check:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    DEVICE = 0  # Use first GPU\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No GPU detected - training will be slower on CPU\")\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "print(f\"\\n‚úÖ Will use device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Download the Dataset\n",
    "\n",
    "We'll download the **Cleveland Pokemon Card** dataset from Roboflow - 5,058 real-world photos!\n",
    "\n",
    "### Why This Dataset?\n",
    "| Feature | Clean Scans (Bad) | Cleveland Dataset (Good) |\n",
    "|---------|-------------------|--------------------------|\n",
    "| Images | ~400 | **5,058** |\n",
    "| Type | Card fills entire frame | Real photos - hands, desks, binders |\n",
    "| Angles | Perfect alignment | Various rotations |\n",
    "| Lighting | Studio lighting | Real-world conditions |\n",
    "| Cards/Image | Always 1 | Often multiple |\n",
    "\n",
    "### YOLO Annotation Format\n",
    "Each image has a `.txt` file with bounding boxes:\n",
    "```\n",
    "class_id  x_center  y_center  width  height\n",
    "0         0.45      0.52      0.30   0.42\n",
    "```\n",
    "All values are normalized (0-1) so they work at any resolution.\n",
    "\n",
    "### You'll Need a Roboflow API Key (Free!)\n",
    "1. Go to https://app.roboflow.com (create free account)\n",
    "2. Click your profile ‚Üí Settings ‚Üí API Key\n",
    "3. Copy the key and paste it below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 2: Download the Dataset\n",
    "# ============================================================\n",
    "\n",
    "from roboflow import Roboflow\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# ‚ö†Ô∏è PASTE YOUR ROBOFLOW API KEY HERE ‚ö†Ô∏è\n",
    "# Get it free at: https://app.roboflow.com/settings/api\n",
    "ROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "# Determine base directory (handle both notebook and script execution)\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path.cwd()\n",
    "else:\n",
    "    # Assume notebook is in notebooks/ folder, so go up one level to project root\n",
    "    BASE_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "\n",
    "# Dataset will be saved in the data folder\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"datasets\"\n",
    "DATASET_NAME = \"pokemon-card-detection\"\n",
    "DATASET_PATH = None\n",
    "\n",
    "# Check if dataset already exists in data folder\n",
    "expected_path = DATA_DIR / DATASET_NAME\n",
    "\n",
    "if expected_path.exists() and (expected_path / \"data.yaml\").exists():\n",
    "    DATASET_PATH = str(expected_path)\n",
    "    \n",
    "    # Validate it's the correct dataset (Cleveland 5K, not a small dataset)\n",
    "    with open(expected_path / \"data.yaml\", 'r') as f:\n",
    "        import yaml\n",
    "        data_config = yaml.safe_load(f)\n",
    "        \n",
    "    # Check if it's the Cleveland dataset\n",
    "    if 'cleveland' in data_config.get('roboflow', {}).get('workspace', '').lower():\n",
    "        # Count images to verify\n",
    "        train_images = list((expected_path / \"train\" / \"images\").glob(\"*.jpg\"))\n",
    "        if len(train_images) > 1000:  # Should have ~4K training images\n",
    "            print(f\"‚úÖ Found Cleveland dataset with {len(train_images):,} training images\")\n",
    "            print(f\"   Location: {DATASET_PATH}\")\n",
    "            print(\"   Skipping download - using cached version\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Found dataset but only {len(train_images)} images (expected 4000+)\")\n",
    "            print(\"   Removing and will re-download...\")\n",
    "            shutil.rmtree(expected_path)\n",
    "            expected_path = None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Found wrong dataset (not Cleveland). Removing...\")\n",
    "        shutil.rmtree(expected_path)\n",
    "        expected_path = None\n",
    "\n",
    "# Download if not found or was wrong dataset\n",
    "if not expected_path or not expected_path.exists():\n",
    "    if ROBOFLOW_API_KEY == \"YOUR_API_KEY_HERE\" or ROBOFLOW_API_KEY is None:\n",
    "        print(\"‚ùå ERROR: Please set your Roboflow API key!\")\n",
    "        print(\"\")\n",
    "        print(\"   1. Go to https://app.roboflow.com (create free account)\")\n",
    "        print(\"   2. Click profile icon ‚Üí Settings ‚Üí API Key\")\n",
    "        print(\"   3. Set ROBOFLOW_API_KEY environment variable\")\n",
    "        print(\"\")\n",
    "        print(\"   Example: export ROBOFLOW_API_KEY='abc123xyz456'\")\n",
    "        DATASET_PATH = None\n",
    "    else:\n",
    "        print(\"üì• Downloading Cleveland Pokemon Card Dataset...\")\n",
    "        print(\"   ‚ö†Ô∏è  IMPORTANT: This is the LARGE dataset (5,058 images)\")\n",
    "        print(\"   Source: https://universe.roboflow.com/cleveland-nahux/pokemon-card-identification\")\n",
    "        print(\"   Workspace: cleveland-nahux\")\n",
    "        print(\"   Project: pokemon-card-identification\") \n",
    "        print(\"   Version: 5\")\n",
    "        print(\"   License: CC BY 4.0\")\n",
    "        print(\"\")\n",
    "        print(\"   Download size: ~2-3 GB\")\n",
    "        print(\"   This may take 5-10 minutes...\")\n",
    "        print(\"\")\n",
    "        \n",
    "        try:\n",
    "            # Create data directory if it doesn't exist\n",
    "            DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Initialize Roboflow\n",
    "            rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "            \n",
    "            # Download the Cleveland dataset (5,058 real-world images)\n",
    "            # ‚ö†Ô∏è Make sure to use the correct workspace and project!\n",
    "            print(\"   Loading workspace 'cleveland-nahux'...\")\n",
    "            workspace = rf.workspace(\"cleveland-nahux\")\n",
    "            \n",
    "            print(\"   Loading project 'pokemon-card-identification'...\")\n",
    "            project = workspace.project(\"pokemon-card-identification\")\n",
    "            \n",
    "            print(\"   Downloading version 5 in YOLOv11 format...\")\n",
    "            # Download in YOLOv11 format\n",
    "            # NOTE: Roboflow ignores the location parameter and downloads to CWD\n",
    "            dataset = project.version(5).download(\"yolov11\")\n",
    "            \n",
    "            # Get actual download location\n",
    "            download_path = Path(dataset.location)\n",
    "            print(f\"   ‚úì Downloaded to: {download_path}\")\n",
    "            \n",
    "            # Move to organized location in data/datasets/\n",
    "            expected_path = DATA_DIR / DATASET_NAME\n",
    "            \n",
    "            if download_path != expected_path:\n",
    "                print(f\"   Moving to: {expected_path}\")\n",
    "                \n",
    "                # Remove destination if it exists\n",
    "                if expected_path.exists():\n",
    "                    shutil.rmtree(expected_path)\n",
    "                \n",
    "                # Move dataset\n",
    "                shutil.move(str(download_path), str(expected_path))\n",
    "                DATASET_PATH = str(expected_path)\n",
    "                print(f\"   ‚úì Dataset organized\")\n",
    "            else:\n",
    "                DATASET_PATH = str(download_path)\n",
    "            \n",
    "            # Verify dataset\n",
    "            train_images = list((expected_path / \"train\" / \"images\").glob(\"*.jpg\"))\n",
    "            print(f\"\\n‚úÖ Dataset ready!\")\n",
    "            print(f\"   Location: {DATASET_PATH}\")\n",
    "            print(f\"   Training images: {len(train_images):,}\")\n",
    "            \n",
    "            if len(train_images) < 1000:\n",
    "                print(f\"\\n   ‚ö†Ô∏è  WARNING: Only {len(train_images)} training images found!\")\n",
    "                print(\"   Expected ~4,000 images from Cleveland dataset\")\n",
    "                print(\"   You may have downloaded the wrong dataset/version\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Download failed: {e}\")\n",
    "            print(\"\")\n",
    "            print(\"   Troubleshooting:\")\n",
    "            print(\"   - Check your API key is correct\")\n",
    "            print(\"   - Verify workspace name: cleveland-nahux\")\n",
    "            print(\"   - Verify project name: pokemon-card-identification\")\n",
    "            print(\"   - Try version 4 if version 5 fails\")\n",
    "            print(\"   - Check internet connection\")\n",
    "            DATASET_PATH = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 2b: Analyze Dataset Statistics\n",
    "# ============================================================\n",
    "\n",
    "import yaml\n",
    "\n",
    "def analyze_dataset(path):\n",
    "    \"\"\"Analyze dataset structure and statistics\"\"\"\n",
    "    print(\"üìä Dataset Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stats = {}\n",
    "    total_images = 0\n",
    "    total_boxes = 0\n",
    "    \n",
    "    for split in ['train', 'valid', 'val', 'test']:\n",
    "        images_dir = os.path.join(path, split, 'images')\n",
    "        labels_dir = os.path.join(path, split, 'labels')\n",
    "        \n",
    "        if not os.path.exists(images_dir):\n",
    "            continue\n",
    "        \n",
    "        # Count images\n",
    "        images = [f for f in os.listdir(images_dir) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        # Count bounding boxes and analyze\n",
    "        split_boxes = 0\n",
    "        boxes_per_image = []\n",
    "        \n",
    "        for img in images:\n",
    "            label_file = os.path.splitext(img)[0] + '.txt'\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    num_boxes = len([l for l in f.readlines() if l.strip()])\n",
    "                    split_boxes += num_boxes\n",
    "                    boxes_per_image.append(num_boxes)\n",
    "        \n",
    "        avg_boxes = sum(boxes_per_image) / len(boxes_per_image) if boxes_per_image else 0\n",
    "        \n",
    "        stats[split] = {\n",
    "            'images': len(images),\n",
    "            'boxes': split_boxes,\n",
    "            'avg_boxes': avg_boxes\n",
    "        }\n",
    "        \n",
    "        total_images += len(images)\n",
    "        total_boxes += split_boxes\n",
    "        \n",
    "        print(f\"\\n{split.upper():6s}:\")\n",
    "        print(f\"   Images: {len(images):,}\")\n",
    "        print(f\"   Bounding boxes: {split_boxes:,}\")\n",
    "        print(f\"   Avg cards/image: {avg_boxes:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TOTAL: {total_images:,} images, {total_boxes:,} bounding boxes\")\n",
    "    \n",
    "    # Load and show data.yaml\n",
    "    data_yaml_path = os.path.join(path, 'data.yaml')\n",
    "    if os.path.exists(data_yaml_path):\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        print(f\"\\nClasses: {data_config.get('names', 'N/A')}\")\n",
    "        print(f\"Number of classes: {data_config.get('nc', 'N/A')}\")\n",
    "    \n",
    "    return total_images, stats\n",
    "\n",
    "if DATASET_PATH:\n",
    "    num_images, dataset_stats = analyze_dataset(DATASET_PATH)\n",
    "else:\n",
    "    print(\"‚ùå Cannot analyze - dataset not downloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualize Training Data\n",
    "\n",
    "**Always look at your data before training!** This helps you:\n",
    "1. Verify annotations are correct\n",
    "2. Understand what the model will learn\n",
    "3. Spot potential issues (bad labels, poor image quality)\n",
    "\n",
    "Let's see some samples with their bounding boxes drawn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 3: Visualize Training Data\n",
    "# ============================================================\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def visualize_samples(dataset_path, num_samples=6):\n",
    "    \"\"\"Show random samples from training set with bounding boxes\"\"\"\n",
    "    \n",
    "    # Find images directory\n",
    "    train_images = os.path.join(dataset_path, 'train', 'images')\n",
    "    train_labels = os.path.join(dataset_path, 'train', 'labels')\n",
    "    \n",
    "    if not os.path.exists(train_images):\n",
    "        print(f\"‚ùå Training images not found at {train_images}\")\n",
    "        return\n",
    "    \n",
    "    # Get all images\n",
    "    all_images = [f for f in os.listdir(train_images) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if len(all_images) == 0:\n",
    "        print(\"‚ùå No images found in training set\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    samples = random.sample(all_images, min(num_samples, len(all_images)))\n",
    "    \n",
    "    # Load class names\n",
    "    data_yaml_path = os.path.join(dataset_path, 'data.yaml')\n",
    "    class_names = ['card']  # Default\n",
    "    if os.path.exists(data_yaml_path):\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            class_names = data.get('names', ['card'])\n",
    "    \n",
    "    # Create grid\n",
    "    rows = 2\n",
    "    cols = 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, img_name in enumerate(samples):\n",
    "        img_path = os.path.join(train_images, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Load and draw bounding boxes\n",
    "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        label_path = os.path.join(train_labels, label_name)\n",
    "        num_boxes = 0\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        xc, yc, bw, bh = map(float, parts[1:5])\n",
    "                        \n",
    "                        # Convert YOLO to pixel coords\n",
    "                        x1 = int((xc - bw/2) * w)\n",
    "                        y1 = int((yc - bh/2) * h)\n",
    "                        x2 = int((xc + bw/2) * w)\n",
    "                        y2 = int((yc + bh/2) * h)\n",
    "                        \n",
    "                        # Draw box\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Add label\n",
    "                        label = class_names[cls_id] if cls_id < len(class_names) else f\"cls_{cls_id}\"\n",
    "                        cv2.putText(img, label, (x1, y1-5), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                        num_boxes += 1\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"{num_boxes} card(s)\", fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Cleveland Dataset Samples ({len(all_images):,} training images)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Showing {len(samples)} random samples from {len(all_images):,} training images\")\n",
    "\n",
    "# Visualize samples\n",
    "if DATASET_PATH:\n",
    "    visualize_samples(DATASET_PATH)\n",
    "else:\n",
    "    print(\"‚ùå Cannot visualize - dataset not downloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Configure Training\n",
    "\n",
    "Before we train, let's understand the key settings:\n",
    "\n",
    "### Key Hyperparameters\n",
    "- **epochs**: How many times the model sees ALL images (like re-reading a textbook)\n",
    "- **batch**: Images processed together - limited by GPU memory (reduce if you get OOM errors)\n",
    "- **imgsz**: Input image size - larger means more detail but slower training\n",
    "- **patience**: Stop early if no improvement for N epochs (saves time!)\n",
    "\n",
    "### Transfer Learning\n",
    "We start from **pre-trained weights** (yolo11n.pt) trained on COCO dataset (80 object types). This gives the model a \"head start\" - it already knows edges, shapes, textures. We just teach it what a Pokemon card looks like.\n",
    "\n",
    "Starting from scratch would need 10x more data and time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 4: Configure Training\n",
    "# ============================================================\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Path to data.yaml (from downloaded dataset)\n",
    "DATA_YAML = os.path.join(DATASET_PATH, 'data.yaml') if DATASET_PATH else None\n",
    "\n",
    "# Training configuration - optimized for 10K dataset with GPU\n",
    "CONFIG = {\n",
    "    # Core settings\n",
    "    'epochs': 100,           # Full training\n",
    "    'batch': 16,             # Good for 8GB GPU\n",
    "    'imgsz': 640,            # Standard YOLO input size\n",
    "    'patience': 20,          # Early stopping patience\n",
    "    'device': 0,             # GPU\n",
    "    \n",
    "    # Augmentation (less aggressive since dataset already has variety)\n",
    "    'degrees': 15,           # Rotation range\n",
    "    'translate': 0.1,        # Translation\n",
    "    'scale': 0.3,            # Scale variation\n",
    "    'fliplr': 0.5,           # Horizontal flip probability\n",
    "    'mosaic': 1.0,           # Mosaic augmentation\n",
    "    \n",
    "    # Optimizer\n",
    "    'optimizer': 'AdamW',\n",
    "    'lr0': 0.001,            # Initial learning rate\n",
    "    'lrf': 0.01,             # Final LR factor\n",
    "    \n",
    "    # Performance (adjusted for cuDNN stability)\n",
    "    'workers': 0,            # Fix cuDNN stream issues\n",
    "    'cache': True,           # RAM cache instead of disk\n",
    "    'amp': False,            # Disable AMP to avoid cuDNN issues\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   {'epochs':12s}: {CONFIG['epochs']}\")\n",
    "print(f\"   {'batch':12s}: {CONFIG['batch']}\")\n",
    "print(f\"   {'imgsz':12s}: {CONFIG['imgsz']}\")\n",
    "print(f\"   {'patience':12s}: {CONFIG['patience']}\")\n",
    "print(f\"   {'device':12s}: {CONFIG['device']}\")\n",
    "print(f\"   {'optimizer':12s}: {CONFIG['optimizer']}\")\n",
    "print(f\"   {'workers':12s}: {CONFIG['workers']} (0 to avoid cuDNN issues)\")\n",
    "print(f\"   {'amp':12s}: {CONFIG['amp']} (disabled for stability)\")\n",
    "print(\"\")\n",
    "print(\"   üéÆ Training on GPU with cuDNN stability fixes\")\n",
    "print(\"   ‚è±Ô∏è  Estimated time: 2-3 hours for 100 epochs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify data.yaml\n",
    "if DATA_YAML and os.path.exists(DATA_YAML):\n",
    "    print(f\"\\nüìÑ Dataset config: {DATA_YAML}\")\n",
    "    print(\"-\" * 60)\n",
    "    with open(DATA_YAML, 'r') as f:\n",
    "        print(f.read())\n",
    "    print(\"-\" * 60)\n",
    "    print(\"‚úÖ Ready to train!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå data.yaml not found - download the dataset first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Train the Model\n",
    "\n",
    "### ‚ö†Ô∏è CUDA \"Illegal Instruction\" Error?\n",
    "\n",
    "If GPU training crashes with this error, it's a **PyTorch/CUDA driver mismatch**.\n",
    "\n",
    "**Quick Fix:** Set `FORCE_CPU = True` in the code cell below\n",
    "\n",
    "**Proper Fix:** Reinstall PyTorch in terminal:\n",
    "```bash\n",
    "pip uninstall torch torchvision torchaudio -y\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "Then restart the kernel and re-run all cells.\n",
    "\n",
    "### ‚úÖ Checkpoint Resume Support\n",
    "\n",
    "**The training cell now supports automatic resume!**\n",
    "\n",
    "- YOLO saves checkpoints every epoch as `last.pt`\n",
    "- If training stops (crash, interrupt, early exit), just **run the cell again**\n",
    "- It will automatically detect the checkpoint and resume from the last epoch\n",
    "- Your progress is never lost!\n",
    "\n",
    "**How it works:**\n",
    "1. First run: Trains from epoch 1\n",
    "2. If interrupted at epoch 10: Next run starts at epoch 11\n",
    "3. Continues until epoch 100 (or early stopping triggers)\n",
    "\n",
    "### Training Settings\n",
    "| Setting | GPU | CPU |\n",
    "|---------|-----|-----|\n",
    "| Epochs | 100 | 50 |\n",
    "| Batch | 16 | 8 |\n",
    "| Cache | RAM | off |\n",
    "| Time | ~2-3 hours | ~4-6 hours |\n",
    "\n",
    "### Check Training Status Before Starting\n",
    "\n",
    "Run the cell below to see if you have an existing checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 5A: HYBRID TRAINING - Local GPU with Colab Fallback\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ THIS CELL DOES BOTH:\n",
    "1. Tries LOCAL GPU first with CUDA error fix\n",
    "2. If it fails 2x, tells you how to switch to Colab\n",
    "3. Your epoch 8 checkpoint is preserved either way!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ‚ö†Ô∏è CUDA FIX: Prevents \"illegal instruction\" errors\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"üîß Applied CUDA_LAUNCH_BLOCKING=1 fix\")\n",
    "\n",
    "# Initialize YOLO11 nano\n",
    "print(\"\\nüöÄ Initializing YOLO11n model...\")\n",
    "\n",
    "# Output directory\n",
    "PROJECT_DIR = '../models/training_runs'\n",
    "RUN_NAME = 'yolo11n_cleveland_notebook'\n",
    "CHECKPOINT_PATH = f'{PROJECT_DIR}/{RUN_NAME}/weights/last.pt'\n",
    "RESUME_TRAINING = os.path.exists(CHECKPOINT_PATH)\n",
    "\n",
    "# Check current environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    print(f\"‚úÖ Found checkpoint: {CHECKPOINT_PATH}\")\n",
    "    \n",
    "    # Check what epoch we're at\n",
    "    results_csv = f'{PROJECT_DIR}/{RUN_NAME}/results.csv'\n",
    "    if os.path.exists(results_csv):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(results_csv)\n",
    "        last_epoch = len(df)\n",
    "        print(f\"   üìà Resuming from epoch {last_epoch} (will start epoch {last_epoch + 1})\")\n",
    "    else:\n",
    "        print(\"   üìà Resuming from checkpoint (epoch unknown)\")\n",
    "    \n",
    "    model = YOLO(CHECKPOINT_PATH)\n",
    "else:\n",
    "    print(\"   Starting fresh training from pretrained weights...\")\n",
    "    model = YOLO('yolo11n.pt')\n",
    "\n",
    "print(f\"\\nüìÅ Results: {PROJECT_DIR}/{RUN_NAME}/\")\n",
    "print(f\"üñ•Ô∏è  Environment: {'Google Colab' if IN_COLAB else 'Local Machine'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê COLAB MODE DETECTED\")\n",
    "    print(\"   Using Colab GPU (T4 or better)\")\n",
    "    print(\"   ‚è±Ô∏è  Estimated: 4-6 hours for remaining epochs\")\n",
    "else:\n",
    "    print(\"üéÆ LOCAL GPU MODE\")\n",
    "    print(\"   Using: NVIDIA GeForce RTX 4070 Laptop GPU\")\n",
    "    print(\"   ‚è±Ô∏è  Estimated: 2-3 hours for remaining epochs\")\n",
    "    print(\"   üîß CUDA fix applied: CUDA_LAUNCH_BLOCKING=1\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training attempt with error handling\n",
    "try:\n",
    "    results = model.train(\n",
    "        data=DATA_YAML,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch=CONFIG['batch'],\n",
    "        imgsz=CONFIG['imgsz'],\n",
    "        patience=CONFIG['patience'],\n",
    "        device=CONFIG['device'],\n",
    "        \n",
    "        # Augmentation\n",
    "        augment=True,\n",
    "        degrees=CONFIG['degrees'],\n",
    "        translate=CONFIG['translate'],\n",
    "        scale=CONFIG['scale'],\n",
    "        fliplr=CONFIG['fliplr'],\n",
    "        mosaic=CONFIG['mosaic'],\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer=CONFIG['optimizer'],\n",
    "        lr0=CONFIG['lr0'],\n",
    "        lrf=CONFIG['lrf'],\n",
    "        \n",
    "        # Project organization\n",
    "        project=PROJECT_DIR,\n",
    "        name=RUN_NAME,\n",
    "        exist_ok=True,\n",
    "        resume=RESUME_TRAINING,\n",
    "        \n",
    "        # Performance settings\n",
    "        workers=CONFIG['workers'],\n",
    "        cache=CONFIG['cache'],\n",
    "        amp=CONFIG['amp'],\n",
    "        \n",
    "        # Logging\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(f\"üìÅ Best model: {PROJECT_DIR}/{RUN_NAME}/weights/best.pt\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚è∏Ô∏è  Training interrupted by user (Ctrl+C)\")\n",
    "    print(f\"üíæ Checkpoint saved: {CHECKPOINT_PATH}\")\n",
    "    print(\"   To resume: Just run this cell again!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    error_msg = str(e).lower()\n",
    "    \n",
    "    if 'cuda' in error_msg and ('illegal' in error_msg or 'error' in error_msg):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚ùå CUDA ERROR DETECTED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Error: {e}\\n\")\n",
    "        \n",
    "        # Check if checkpoint exists\n",
    "        if os.path.exists(CHECKPOINT_PATH):\n",
    "            checkpoint_size = os.path.getsize(CHECKPOINT_PATH) / (1024**2)\n",
    "            print(\"‚úÖ YOUR PROGRESS IS SAFE!\")\n",
    "            print(f\"   Checkpoint: {CHECKPOINT_PATH}\")\n",
    "            print(f\"   Size: {checkpoint_size:.1f} MB\")\n",
    "            \n",
    "            # Check epoch count\n",
    "            if os.path.exists(f'{PROJECT_DIR}/{RUN_NAME}/results.csv'):\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(f'{PROJECT_DIR}/{RUN_NAME}/results.csv')\n",
    "                print(f\"   Completed epochs: {len(df)}\")\n",
    "                print(f\"   Next resume: Epoch {len(df) + 1}\")\n",
    "        \n",
    "        print(\"\\nüîß FIX OPTIONS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"\\nüìç OPTION 1: Try reducing batch size (Quick)\")\n",
    "        print(\"   In cell above (Section 4), change:\")\n",
    "        print(\"   CONFIG['batch'] = 8  # Was 16\")\n",
    "        print(\"   Then re-run this cell\")\n",
    "        \n",
    "        print(\"\\nüìç OPTION 2: Reinstall PyTorch (Proper fix)\")\n",
    "        print(\"   Open terminal and run:\")\n",
    "        print(\"   conda activate pricelens\")\n",
    "        print(\"   pip uninstall torch torchvision torchaudio -y\")\n",
    "        print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "        print(\"   Then restart Jupyter kernel and re-run this cell\")\n",
    "        \n",
    "        print(\"\\nüìç OPTION 3: Switch to Google Colab (Easiest)\")\n",
    "        print(\"   See the next cell below for Colab migration instructions!\")\n",
    "        print(\"   Your checkpoint will be uploaded and training continues there.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        raise\n",
    "        \n",
    "    else:\n",
    "        # Other error\n",
    "        print(f\"\\n‚ùå Training failed: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\\n\")\n",
    "        \n",
    "        if os.path.exists(CHECKPOINT_PATH):\n",
    "            print(f\"üíæ Checkpoint exists: {CHECKPOINT_PATH}\")\n",
    "            print(\"   You can resume by running this cell again\")\n",
    "        \n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\\n\")\n",
    "    \n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"üíæ Checkpoint exists: {CHECKPOINT_PATH}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Update for later cells\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/{RUN_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Colab Checkpoint Restore (Run this in Colab ONLY)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê Google Colab detected!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üì§ Upload your checkpoint package...\")\n",
    "    print(\"   Click 'Choose Files' and select: colab_migration_checkpoint.zip\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    # Upload the migration package\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if 'colab_migration_checkpoint.zip' in uploaded:\n",
    "        print(\"\\n‚úÖ Checkpoint package received!\")\n",
    "        print(\"üì¶ Extracting...\")\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        PROJECT_DIR = '../models/training_runs'\n",
    "        RUN_NAME = 'yolo11n_cleveland_notebook'\n",
    "        CHECKPOINT_DIR = f'{PROJECT_DIR}/{RUN_NAME}'\n",
    "        \n",
    "        os.makedirs(f'{CHECKPOINT_DIR}/weights', exist_ok=True)\n",
    "        \n",
    "        # Extract checkpoint\n",
    "        with zipfile.ZipFile('colab_migration_checkpoint.zip', 'r') as zf:\n",
    "            # Extract last.pt to checkpoint location\n",
    "            if 'last.pt' in zf.namelist():\n",
    "                zf.extract('last.pt', f'{CHECKPOINT_DIR}/weights/')\n",
    "                print(\"   ‚úì Restored checkpoint: last.pt\")\n",
    "            \n",
    "            # Extract results.csv if present\n",
    "            if 'results.csv' in zf.namelist():\n",
    "                zf.extract('results.csv', CHECKPOINT_DIR)\n",
    "                print(\"   ‚úì Restored training history: results.csv\")\n",
    "                \n",
    "                # Show progress\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(f'{CHECKPOINT_DIR}/results.csv')\n",
    "                last_epoch = len(df)\n",
    "                print(f\"\\nüìà Restored Progress:\")\n",
    "                print(f\"   Completed epochs: {last_epoch}/100\")\n",
    "                print(f\"   Will resume from epoch: {last_epoch + 1}\")\n",
    "            \n",
    "            # Extract best.pt if present\n",
    "            if 'best.pt' in zf.namelist():\n",
    "                zf.extract('best.pt', f'{CHECKPOINT_DIR}/weights/')\n",
    "                print(\"   ‚úì Restored best weights: best.pt\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ CHECKPOINT RESTORED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nüëâ Now run the training cell above (Section 5A)\")\n",
    "        print(\"   It will automatically resume from your checkpoint!\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\n‚ùå Checkpoint package not found\")\n",
    "        print(\"   Please upload: colab_migration_checkpoint.zip\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  This cell is for Colab only\")\n",
    "    print(\"   Running locally - skip this cell\")\n",
    "    print(\"   Your checkpoint is already in the right place!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Colab Migration Helper - Package Your Checkpoint\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = '../models/training_runs'\n",
    "RUN_NAME = 'yolo11n_cleveland_notebook'\n",
    "CHECKPOINT_DIR = f'{PROJECT_DIR}/{RUN_NAME}'\n",
    "\n",
    "print(\"üì¶ Creating Colab Migration Package\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_path = f'{CHECKPOINT_DIR}/weights/last.pt'\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(\"‚ùå No checkpoint found!\")\n",
    "    print(f\"   Expected: {checkpoint_path}\")\n",
    "    print(\"\\n   Nothing to migrate. Start training first.\")\n",
    "else:\n",
    "    # Get checkpoint info\n",
    "    checkpoint_size = os.path.getsize(checkpoint_path) / (1024**2)\n",
    "    print(f\"‚úÖ Checkpoint found: {checkpoint_path}\")\n",
    "    print(f\"   Size: {checkpoint_size:.1f} MB\")\n",
    "    \n",
    "    # Check progress\n",
    "    results_csv = f'{CHECKPOINT_DIR}/results.csv'\n",
    "    if os.path.exists(results_csv):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(results_csv)\n",
    "        last_epoch = len(df)\n",
    "        latest_metrics = df.iloc[-1]\n",
    "        \n",
    "        print(f\"\\nüìà Current Training Progress:\")\n",
    "        print(f\"   Completed epochs: {last_epoch}/100\")\n",
    "        print(f\"   Progress: {last_epoch}%\")\n",
    "        \n",
    "        if 'metrics/mAP50(B)' in df.columns:\n",
    "            map50 = latest_metrics['metrics/mAP50(B)']\n",
    "            print(f\"   Latest mAP@50: {map50:.4f}\")\n",
    "    \n",
    "    # Create migration package\n",
    "    print(f\"\\nüìÅ Creating migration ZIP...\")\n",
    "    migration_zip = 'colab_migration_checkpoint.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(migration_zip, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Add checkpoint\n",
    "        zf.write(checkpoint_path, 'last.pt')\n",
    "        print(f\"   ‚úì Added checkpoint\")\n",
    "        \n",
    "        # Add results.csv if exists\n",
    "        if os.path.exists(results_csv):\n",
    "            zf.write(results_csv, 'results.csv')\n",
    "            print(f\"   ‚úì Added training history\")\n",
    "        \n",
    "        # Add best weights if they exist\n",
    "        best_path = f'{CHECKPOINT_DIR}/weights/best.pt'\n",
    "        if os.path.exists(best_path):\n",
    "            zf.write(best_path, 'best.pt')\n",
    "            print(f\"   ‚úì Added best weights\")\n",
    "    \n",
    "    zip_size = os.path.getsize(migration_zip) / (1024**2)\n",
    "    print(f\"\\n‚úÖ Migration package created!\")\n",
    "    print(f\"   File: {migration_zip}\")\n",
    "    print(f\"   Size: {zip_size:.1f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üì§ NEXT STEPS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n1Ô∏è‚É£  Upload this notebook to Google Colab:\")\n",
    "    print(\"   ‚Ä¢ Go to https://colab.research.google.com\")\n",
    "    print(\"   ‚Ä¢ File ‚Üí Upload notebook\")\n",
    "    print(\"   ‚Ä¢ Upload: train_card_detector.ipynb\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£  In Colab, upload the migration package:\")\n",
    "    print(\"   ‚Ä¢ Run the cell below in Colab\")\n",
    "    print(\"   ‚Ä¢ It will prompt you to upload the ZIP file\")\n",
    "    print(f\"   ‚Ä¢ Upload: {migration_zip}\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£  Continue training from epoch\", last_epoch + 1 if os.path.exists(results_csv) else \"?\")\n",
    "    print(\"   ‚Ä¢ Just run the training cell!\")\n",
    "    print(\"   ‚Ä¢ It will automatically resume\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üìÅ Your file is ready: {os.path.abspath(migration_zip)}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5B: Google Colab Migration (If Local GPU Fails)\n",
    "\n",
    "If the above cell keeps failing with CUDA errors, **switch to Google Colab** and continue from your checkpoint!\n",
    "\n",
    "### üéØ Your Training Will Resume from Epoch 8!\n",
    "\n",
    "The checkpoint system works perfectly across different machines. Here's how to migrate:\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Prepare Checkpoint Package\n",
    "\n",
    "Run the cell below to create a migration package with your checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Check Training Status and Checkpoints\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_DIR = '../models/training_runs'\n",
    "RUN_NAME = 'yolo11n_cleveland_notebook'\n",
    "CHECKPOINT_PATH = f'{PROJECT_DIR}/{RUN_NAME}/weights/last.pt'\n",
    "RESULTS_PATH = f'{PROJECT_DIR}/{RUN_NAME}/results.csv'\n",
    "\n",
    "print(\"üìä Training Status Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    # Get checkpoint info\n",
    "    checkpoint_size = os.path.getsize(CHECKPOINT_PATH) / (1024 * 1024)\n",
    "    checkpoint_time = os.path.getmtime(CHECKPOINT_PATH)\n",
    "    from datetime import datetime\n",
    "    checkpoint_date = datetime.fromtimestamp(checkpoint_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint found!\")\n",
    "    print(f\"   Path: {CHECKPOINT_PATH}\")\n",
    "    print(f\"   Size: {checkpoint_size:.1f} MB\")\n",
    "    print(f\"   Last updated: {checkpoint_date}\")\n",
    "    \n",
    "    # Check results.csv to see progress\n",
    "    if os.path.exists(RESULTS_PATH):\n",
    "        df = pd.read_csv(RESULTS_PATH)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        last_epoch = len(df)\n",
    "        latest = df.iloc[-1]\n",
    "        \n",
    "        print(f\"\\nüìà Training Progress:\")\n",
    "        print(f\"   Completed epochs: {last_epoch}/100\")\n",
    "        print(f\"   Progress: {last_epoch}%\")\n",
    "        print(f\"   Latest mAP@50: {latest.get('metrics/mAP50(B)', 'N/A'):.4f}\")\n",
    "        print(f\"   Latest mAP@50-95: {latest.get('metrics/mAP50-95(B)', 'N/A'):.4f}\")\n",
    "        \n",
    "        if last_epoch < 100:\n",
    "            print(f\"\\nüîÑ Next run will resume from epoch {last_epoch + 1}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Training appears complete!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No results.csv found - checkpoint may be from initial setup\")\n",
    "        print(\"   Will start training from beginning\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found\")\n",
    "    print(\"   Training will start fresh from epoch 1\")\n",
    "    print(\"   Using pretrained YOLO11n weights as starting point\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Evaluate Results\n",
    "\n",
    "Now let's see how well our model trained! We'll look at:\n",
    "\n",
    "### Training Curves\n",
    "- **Loss curves**: Should decrease over time (model is learning)\n",
    "- **Metric curves**: mAP, precision, recall should increase\n",
    "\n",
    "### What the metrics mean\n",
    "- **Precision**: Of all boxes the model drew, what % were correct?\n",
    "- **Recall**: Of all actual cards, what % did the model find?\n",
    "- **mAP@50**: Average precision at 50% IoU overlap (main metric)\n",
    "- **mAP@50-95**: Stricter - averaged across IoU 50% to 95%\n",
    "\n",
    "Good results for card detection:\n",
    "- mAP@50 > 0.85 = Good\n",
    "- mAP@50 > 0.90 = Great\n",
    "- mAP@50 > 0.95 = Excellent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 6: Evaluate Results\n",
    "# ============================================================\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import pandas as pd\n",
    "\n",
    "# Results directory from training\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/{RUN_NAME}'  # pokemon_detector/yolo11n_cleveland_5k\n",
    "\n",
    "print(\"üìä Training Results Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show training curves\n",
    "print(\"\\nüìà Training Curves:\")\n",
    "curves_path = os.path.join(RESULTS_DIR, 'results.png')\n",
    "if os.path.exists(curves_path):\n",
    "    display(Image(filename=curves_path, width=900))\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Curves not found at {curves_path}\")\n",
    "\n",
    "# Show confusion matrix if available\n",
    "print(\"\\nüéØ Confusion Matrix:\")\n",
    "cm_path = os.path.join(RESULTS_DIR, 'confusion_matrix.png')\n",
    "if os.path.exists(cm_path):\n",
    "    display(Image(filename=cm_path, width=600))\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Confusion matrix not generated\")\n",
    "\n",
    "# Show sample predictions on validation set\n",
    "print(\"\\nüñºÔ∏è  Sample Predictions on Validation Set:\")\n",
    "val_preds = os.path.join(RESULTS_DIR, 'val_batch0_pred.jpg')\n",
    "if os.path.exists(val_preds):\n",
    "    display(Image(filename=val_preds, width=800))\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Validation predictions not found\")\n",
    "\n",
    "# Print final metrics from CSV\n",
    "print(\"\\nüìã Final Training Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "metrics_path = os.path.join(RESULTS_DIR, 'results.csv')\n",
    "\n",
    "if os.path.exists(metrics_path):\n",
    "    df = pd.read_csv(metrics_path)\n",
    "    df.columns = df.columns.str.strip()  # Remove whitespace from column names\n",
    "    \n",
    "    # Get the last (best) epoch\n",
    "    final = df.iloc[-1]\n",
    "    \n",
    "    # Metrics to display\n",
    "    metrics_to_show = [\n",
    "        ('metrics/precision(B)', 'Precision'),\n",
    "        ('metrics/recall(B)', 'Recall'),\n",
    "        ('metrics/mAP50(B)', 'mAP@50'),\n",
    "        ('metrics/mAP50-95(B)', 'mAP@50-95'),\n",
    "        ('train/box_loss', 'Final Box Loss'),\n",
    "        ('train/cls_loss', 'Final Class Loss'),\n",
    "    ]\n",
    "    \n",
    "    for col, name in metrics_to_show:\n",
    "        if col in final:\n",
    "            value = final[col]\n",
    "            # Add emoji indicators\n",
    "            if 'mAP50(B)' in col:\n",
    "                if value > 0.95:\n",
    "                    indicator = \"üåü Excellent!\"\n",
    "                elif value > 0.90:\n",
    "                    indicator = \"‚úÖ Great\"\n",
    "                elif value > 0.85:\n",
    "                    indicator = \"üëç Good\"\n",
    "                else:\n",
    "                    indicator = \"üìà Needs more training\"\n",
    "                print(f\"   {name:20s}: {value:.4f}  {indicator}\")\n",
    "            else:\n",
    "                print(f\"   {name:20s}: {value:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   Total epochs trained: {len(df)}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Metrics file not found at {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Test the Model\n",
    "\n",
    "Let's load our trained model and run it on some test images to see it in action!\n",
    "\n",
    "The model outputs:\n",
    "- **Bounding boxes**: Coordinates of detected cards\n",
    "- **Confidence scores**: How sure the model is (0-1)\n",
    "- **Class labels**: What class was detected (pokemon_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 7: Test the Model\n",
    "# ============================================================\n",
    "\n",
    "# Load the best trained model\n",
    "BEST_MODEL_PATH = f'{RESULTS_DIR}/weights/best.pt'\n",
    "\n",
    "print(\"üîÑ Loading trained model...\")\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    best_model = YOLO(BEST_MODEL_PATH)\n",
    "    print(f\"‚úÖ Loaded: {BEST_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found: {BEST_MODEL_PATH}\")\n",
    "    print(\"   Make sure training completed successfully\")\n",
    "    best_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation images\n",
    "if best_model and DATASET_PATH:\n",
    "    # Find validation images (try both 'valid' and 'val' naming)\n",
    "    val_images_dir = os.path.join(DATASET_PATH, 'valid', 'images')\n",
    "    if not os.path.exists(val_images_dir):\n",
    "        val_images_dir = os.path.join(DATASET_PATH, 'val', 'images')\n",
    "    \n",
    "    if os.path.exists(val_images_dir):\n",
    "        all_val_images = [f for f in os.listdir(val_images_dir) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Random sample of 6 images\n",
    "        test_images = random.sample(all_val_images, min(6, len(all_val_images)))\n",
    "        \n",
    "        print(f\"üß™ Testing on {len(test_images)} random validation images...\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for ax, img_name in zip(axes, test_images):\n",
    "            img_path = os.path.join(val_images_dir, img_name)\n",
    "            \n",
    "            # Run inference\n",
    "            results = best_model.predict(img_path, conf=0.5, verbose=False)\n",
    "            \n",
    "            # Get annotated image\n",
    "            annotated = results[0].plot()\n",
    "            annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Count detections\n",
    "            num_detections = len(results[0].boxes)\n",
    "            \n",
    "            ax.imshow(annotated)\n",
    "            ax.set_title(f'{num_detections} card(s) detected', fontsize=10)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle('Model Predictions on Validation Images (Cleveland 5K)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detection details for last image\n",
    "        print(\"\\nüìã Detection details (last image):\")\n",
    "        for i, box in enumerate(results[0].boxes):\n",
    "            conf = float(box.conf[0])\n",
    "            cls = int(box.cls[0])\n",
    "            cls_name = best_model.names[cls]\n",
    "            bbox = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            print(f\"   Card {i+1}: {cls_name} (conf={conf:.3f}) at [{bbox[0]}, {bbox[1]}, {bbox[2]}, {bbox[3]}]\")\n",
    "    else:\n",
    "        print(f\"‚ùå Validation images not found\")\n",
    "        print(f\"   Checked: {val_images_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Export for PriceLens\n",
    "\n",
    "Now let's export our trained model so it can be used with PriceLens!\n",
    "\n",
    "### What You'll Get\n",
    "- A `.pt` file containing the trained model weights\n",
    "- In Colab: Automatic download to your computer\n",
    "- Locally: File ready in the current directory\n",
    "\n",
    "### Integration Steps\n",
    "After downloading the model:\n",
    "1. Copy the `.pt` file to your PriceLens `models/` directory\n",
    "2. Update `config.yaml` with the new model path\n",
    "3. Set `use_card_specific_model: true` (important!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 8: Export for PriceLens\n",
    "# ============================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Export filename - includes dataset info for clarity\n",
    "EXPORT_NAME = 'pokemon_card_detector_5k.pt'\n",
    "\n",
    "print(\"üì¶ Exporting model for PriceLens...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    # Copy to current directory with a clear name\n",
    "    shutil.copy(BEST_MODEL_PATH, EXPORT_NAME)\n",
    "    \n",
    "    # Get model info\n",
    "    model_size_mb = os.path.getsize(EXPORT_NAME) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"‚úÖ Model exported: {EXPORT_NAME}\")\n",
    "    print(f\"   Size: {model_size_mb:.2f} MB\")\n",
    "    print(f\"   Classes: {best_model.names}\")\n",
    "    print(f\"   Trained on: Cleveland 5K real-world dataset\")\n",
    "    \n",
    "    # Download in Colab\n",
    "    if IN_COLAB:\n",
    "        print(\"\\nüì• Starting download...\")\n",
    "        from google.colab import files\n",
    "        files.download(EXPORT_NAME)\n",
    "        print(\"   Download started! Check your browser downloads.\")\n",
    "    else:\n",
    "        export_path = os.path.abspath(EXPORT_NAME)\n",
    "        print(f\"\\nüìÅ Model saved to: {export_path}\")\n",
    "        \n",
    "        # Also copy to models directory if we're in the project\n",
    "        models_dir = \"../models\"\n",
    "        if os.path.exists(models_dir):\n",
    "            dest = os.path.join(models_dir, EXPORT_NAME)\n",
    "            shutil.copy(EXPORT_NAME, dest)\n",
    "            print(f\"   Also copied to: {os.path.abspath(dest)}\")\n",
    "else:\n",
    "    print(f\"‚ùå Best model not found at {BEST_MODEL_PATH}\")\n",
    "    print(\"   Make sure training completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with PriceLens\n",
    "\n",
    "Congratulations! You've trained a Pokemon card detector on **5,058 real-world images**!\n",
    "\n",
    "### Step 1: Copy the Model\n",
    "Move `pokemon_card_detector_5k.pt` to your PriceLens `models/` directory:\n",
    "```bash\n",
    "cp pokemon_card_detector_5k.pt /path/to/PriceLens/models/\n",
    "```\n",
    "\n",
    "### Step 2: Update config.yaml\n",
    "Edit your `config.yaml` file:\n",
    "```yaml\n",
    "detection:\n",
    "  model_path: \"models/pokemon_card_detector_5k.pt\"\n",
    "  use_card_specific_model: true  # IMPORTANT! Disables COCO class filtering\n",
    "  confidence_threshold: 0.5\n",
    "```\n",
    "\n",
    "### Step 3: Test It!\n",
    "```bash\n",
    "# From PriceLens directory\n",
    "python scripts/test_detector_standalone.py\n",
    "```\n",
    "\n",
    "Or run the full web app:\n",
    "```bash\n",
    "python run_web.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "With the Cleveland 5K dataset, you should see:\n",
    "- **mAP@50**: 0.85+ (vs ~0.04 with clean scans!)\n",
    "- **mAP@50-95**: 0.65+\n",
    "- Robust detection in various lighting\n",
    "- Handles multiple cards per frame\n",
    "- Works with cards in hands, on desks, in binders\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Model doesn't detect cards well?**\n",
    "- Lower confidence_threshold to 0.3 in config.yaml\n",
    "- Check if lighting is too dark/bright\n",
    "- Ensure cards are visible (not too small in frame)\n",
    "\n",
    "**Out of memory during training?**\n",
    "- Reduce batch size: `CONFIG['batch'] = 8` or `4`\n",
    "- Reduce image size: `CONFIG['imgsz'] = 416`\n",
    "- Disable caching: `CONFIG['cache'] = False`\n",
    "\n",
    "**Training is slow?**\n",
    "- In Colab: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "- Expected: ~1-2 hours for 100 epochs on T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Attribution\n",
    "\n",
    "Cleveland Pokemon Card Dataset - CC BY 4.0\n",
    "https://universe.roboflow.com/cleveland-nahux/pokemon-card-identification\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Now that detection works, PriceLens can:\n",
    "1. **Identify** which card it is (using `card_database/` for feature matching)\n",
    "2. **Look up prices** from APIs\n",
    "3. **Display** the price overlay\n",
    "\n",
    "Check out the other notebooks and documentation in the PriceLens repo!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pricelens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
